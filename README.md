# Data-Pipelining-using-Postgres-and-Python
#### **Project Overview:**
<div style="text-align: justify">
Sparkify is a music streaming platform which lets user to listen to music for a fixed monthly premium.It has dedicated repository of music from which the user
can choose from. Sparkify wants to improve their product and to make informed
decisions based on the log data generated by the user every time a song is
played. They want to build an OLAP system which the Data Scientists and Data Analysts use to mine the data. The purpose of this entire project is to design
a OLAP data warehouse which can be efficiently analyzed for insights.
</div>

#### **Database Schema Design:**
<div style="text-align: justify">
In this project we have gone ahead with a star schema with one fact table and four dimension tables. We have chose star schema because unlike snowflake schema, it is denormalized which makes the analysts use less joins which in turn means higher efficiency of the queries.
</div>
<br>
<div style="text-align: justify">
Songplays(Fact Table): records in log data associated with song plays i.e. records with page 'NextSong'
</div>
<br>
<div style="text-align: justify">
Users(Dim Table): users using Sparkify app <br>
Songs(Dim Table): songs in the Sparkify Database <br>
Artists(Dim Table): Artists in the Sparkify Database <br>
Time(Dim Table): Timestamps of records in 'songplays' broken down to specific units 
</div>

#### **ETL Pipeline Design:**

Our ETL process consists of three file:
  1. sql_queries.py
  2. create_tables.py
  3. etl.py

###### **sql_queries.py:**
<div style="text-align: justify">
This is the python script which consists of sql drop statements and create statements which is utilized by the create_tables.py script to physically drop and create tables in our postgreSQL database. It also consists of insert statements and search queries which are utilized by the etl.py script to load data into the tables from the json files in our system. 
</div>

###### **create_tables.py:**
<div style="text-align: justify">
This python script creates a new database and connects to it using the credentials. Then it drops the tables if it already exists and then it creates new tables. The create and drop statements are passed to this script from the sql_queries.py script. Running this table would fetch us the empty fact and dimension tables.
</div>

###### **etl.py:**
<div style="text-align: justify">
This python scripts is used to load data into the fact and dimension tables from the song_data and log_data json files. For the insert of users, songs, artists tables , I have implemented a logic such that if a record is already present in a table, then the duplicate incoming record will not be inserted.
</div>
